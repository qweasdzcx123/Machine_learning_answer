{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2** 西瓜数据集3.0a上分别用线性核和高斯核训练一个SVM，并比较支持向量的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 52.9412% (9/17) (classification)\n",
      "[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0] (52.94117647058824, 1.8823529411764706, nan) [[-0.5074720076394073], [-0.5544980076394078], [-0.7069860076394064], [-0.6680600076394079], [-0.7798970076394082], [-0.816729007639408], [-0.8669810076394076], [-0.8275230076394082], [-0.8506730076394085], [-0.8489590076394082], [-1.0370090076394083], [-0.962991007639408], [-0.7977330076394085], [-0.757810007639408], [-0.7130720076394076], [-0.9217340076394083], [-0.8202750076394082]]\n",
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 52.9412% (9/17) (classification)\n",
      "[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0] (52.94117647058824, 1.8823529411764706, nan)\n"
     ]
    }
   ],
   "source": [
    "from libsvm.svmutil import *\n",
    "import numpy as np\n",
    "A = np.array([[0.697,0.460,1],[0.774,0.376,1],[0.634,0.264,1],[0.608,0.318,1],[0.556,0.215,1],\n",
    "     [0.403,0.237,1],[0.481,0.149,1],[0.437,0.211,1],[0.666,0.091,-1],[0.243,0.267,-1],\n",
    "     [0.245,0.057,-1],[0.343,0.099,-1],[0.639,0.161,-1],[0.657,0.198,-1],[0.360,0.370,-1],\n",
    "     [0.593,0.042,-1],[0.719,0.103,-1]])\n",
    "y_train= A[:,2]\n",
    "y_test=A[:,2]\n",
    "x_train = []\n",
    "x_test=[]\n",
    "for i in range(len(y_train)):\n",
    "    dic = {}\n",
    "    dic[1] = A[i,0]\n",
    "    dic[2] = A[i,1]\n",
    "    x_train.append(dic)\n",
    "for i in range(len(y_test)):\n",
    "    dic = {}\n",
    "    dic[1] = A[i,0]\n",
    "    dic[2] = A[i,1]\n",
    "    x_test.append(dic)\n",
    "lina_options = '-t 0 -c 1 -b 1'      #线性核\n",
    "guass_options = '-t 2 -c 1 -b 1'     # 高斯核\n",
    "model = svm_train(y_train,x_train,lina_options)\n",
    "p_label, p_acc, p_val = svm_predict(y_test, x_test, model)     # 使用得到的模型进行预测\n",
    "print(p_label,p_acc,p_val)\n",
    "svm_save_model('./xiGua3.3alpha_linear',model)\n",
    "model_ = svm_train(y_train,x_train ,guass_options)\n",
    "p_label, p_acc, p_val = svm_predict(y_test, x_test, model_)     # 使用得到的模型进行预测\n",
    "print(p_label,p_acc)\n",
    "svm_save_model('./xiGua3.3alpha_Guass',model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "from libsvm.svmutil import *\n",
    "    \n",
    "y, x = [1,-1], [{1:1, 2:1}, {1:-1,2:-1}]       # 输入的数据\n",
    "options = '-t 0 -c 4 -b 1'                     # 训练参数设置\n",
    "model = svm_train(y,x,options)                 # 进行训练\n",
    " \n",
    "yt = [1]\n",
    "xt = [{1:1, 2:1}]\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)     # 使用得到的模型进行预测\n",
    "print(p_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3** 用UCI数据集，分别用高斯核和线性核进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 100% (70/70) (classification)\n",
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 100% (70/70) (classification)\n",
      "线性预测结果\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0] (100.0, 0.0, 1.0)\n",
      "高斯预测结果\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0] (100.0, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "from svm6_3 import main ##iris数据集\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.4**讨论线性判别分析和线性核支持向量机在何种条件下等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. LDA和线性SVM都希望能最大化异类样例间距，但LDA是异类中心间距最大化，而线性SVM考虑的是支持向量间距最大\n",
    "\n",
    "     2. LDA的目标函数考虑了同类样例的协方差，希望同类样例在投影空间尽可能靠近，而线性SVM却没有考虑这一点。\n",
    "\n",
    "     3.关于数据是否线性可分的问题，如果使用软间隔的线性SVM，线性可分这个条件是不必要的，如果是硬间隔线性SVM，那么线性可分是必要条件。但是LDA不管数据是否线性可分，都可以进行处理\n",
    "\n",
    "     4. 假如当前样本线性可分，且SVM与LDA求出的结果相互垂直。则当SVM的支持向量固定时，再加入新的非支持向量样本，并不会改变SVM中求出的w。但是新加入的样本会改变原类型数据的协方差和均值，从而导致LDA求出的结果发生改变。这个时候两者的w就不再垂直，但是数据依然是可分的。所以， 线性可分  和   LDA求出的wl与线性核支持向量机求出的ws垂直，这两个条件是不等价的。\n",
    "\n",
    "     5. 所以，该题的答案严格上来讲，应该是当线性SVM和LDA求出的w互相垂直时，两者是等价的。因为一般LDA是不带偏置项的（因为LDA的思想是投影，投影过程和偏置是没有任何关系的），所以SVM这个时候比LDA仅仅多了个偏移b。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.5** 高斯核SVM和RBF之间的联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人整理如下几点：\n",
    "\n",
    "      1、RBF网络的径向基函数与SVM都采用高斯核，就分别得到了高斯核RBF网络与高斯核SVM。\n",
    "\n",
    "      2、神经网络是最小化累计误差，将参数 w 作为惩罚项；而SVM相反，主要是最小化参数，将误差作为惩罚项。\n",
    "\n",
    "      3、根据书上给出的公式，可以发现，在二分类问题中，如果将RBF中隐层神经元的个数设置为总样本个数，且每个隐层神经元所对应的中心设置为样本参数，并将RBF的径向基函数中的 beta 设置成高斯函数中对应的系数，则得出的RBF网络与核SVM基本等价，非支持向量在RBF中将得到很小的权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **课后题6.6**  SVM对噪声敏感的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人整理如下：\n",
    "\n",
    "        1、SVM的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束（即函数间隔要大于1）\n",
    "\n",
    "        2、当数据集中存在噪声点但是仍然满足线性可分的条件时，SVM为了把噪声点也划分正确，超平面就会向另外一个类的样本靠拢，这就使得划分超平面的几何间距变小，从而降低了模型的泛化性能。\n",
    "\n",
    "        3、当数据集因为存在噪声点而导致已经无法线性可分时，此时就使用了核技巧，通过将样本映射到高维特征空间使得样本线性可分，这样就会得到一个复杂模型，并由此导致过拟合（原样本空间得到的划分超平面会是弯弯曲曲的，它确实可以把所有样本都划分正确，但得到的模型只对训练集有效），泛化能力极差。\n",
    "\n",
    "        4、所以说，SVM对于噪声很敏感。因此，提出了软间隔SVM来防止由于噪声的存在而导致的过拟合问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **课后题6.7**  给出式6.52的完整KKT条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 首先，根据KKT条件的定义，可以知道，对于拉格朗日函数中的带有拉格朗日乘子的项，每一项对应的KKT条件共有三个式子。在式6.52中，带有拉格朗日乘子的共有四项，所以完整的KKT条件应该共有12个式子。\n",
    "\n",
    "#### 完整的KKT条件如下所示：\n",
    "#### ![](https://img-blog.csdnimg.cn/20190102215519582.jpg)\n",
    "#### ![](https://img-blog.csdnimg.cn/20190102215549601.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NjkxOTA5,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **课后题6.8** 以密度为输入，含糖率为输出，训练一个SVR。 \n",
    "\n",
    " 采用 epsilon-SVR进行回归，核函数采用高斯核，具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libsvm.svmutil import *\n",
    "import numpy as np\n",
    "A = np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n",
    "     [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n",
    "     [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n",
    "     [0.593,0.042],[0.719,0.103]])\n",
    "y = A[:,1]\n",
    "x = []\n",
    "for i in range(len(y)):\n",
    "    dic = {}\n",
    "    dic[1] = A[i,0]\n",
    "    x.append(dic)\n",
    " \n",
    "guass_options = '-s 3 -t 2 -c 1 -b 1'     # 高斯核\n",
    "model = svm_train(y,x,guass_options)\n",
    "svm_save_model('./problem_6.8_XG3.3alpha_guass',model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
